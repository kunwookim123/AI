# 경사 하강법
# 가장 낮은 곳을 찾아가는 방법

# 안개 낀 산에서 내려오기
# 앤개 낀 산 정상 -> 마을은 가장 낮은 곳에 있는데 앞이 보이지 않아 어디로 가야할 지 모름

# 발밑의 지형만 느낌녀서 지금 서있는 곳에서 가장 가파르게 내려가는 방향을 찾아 한걸음씩 내딛는 것

# 머신러닝에서 모델을 학습시킨다는 건 최적의 파라미터 값을 찾는다는 의미
# 예를 들면 집 가격을 예측하는 모델이 있다면, 예측값과 실제 가격의 차이(오차)를 최소화하는 파라미터를 찾아야 함

# 이 오차를 수치화한 것이 손실 함수 이고 경사 하강법은 이 손실 함수의 값을 최소화하는 파라미터를 찾아가는 알고리즘

# 작동 원리
# 경사 하강법
# 
# θ(세타): 우리가 찾고자 하는 파라미터
# α(알파): 학습률, 한 번에 얼마나 크게 이동할 지 결정
# ∇L: 현재 위치에서 손실 함수의 기울기(gradient)

# 기울기에 마이너스를 붙이는 이유? 기울기는 함수가 증가하는 방향으로 가리키기 때문에 반대로 가야 감소하는 방향이 되기 때문

# 학습률의 중요성
# 학습률 α는 매우 중요한 하이퍼파라미터

# 학습률이 너무 작으면 수렴하는 데너무 오래 걸림
# 아주 작은 걸음으로 조금씩 이동하니 시간이 한참 걸림

# 한계점
# 지역 최솟값 문제
# 경사 하강법은 현재 위치에서 가장 가파른 방향만 보고 이동하기 때문에
# 전체에서 가장 낮은곳(전역 최솟값, Globla Minimum)이 아니라 근처의 움푹 파인 곳
# (지역 최솟값, Local Minimum)에 갇힐 수 있음

# SGD(확률적 경사 하강법): 전체 데이터 대신 일부 샘플만 사용해서 더 빠르게 업데이트하고, 약간의 무작위성이 지역 최솟값 탈출에 도움을 줌

# Momentum : 이전 이동 방햐의 관성을 반영해서 지역 최솟값을 넘어갈 수 있게 해줌

# Adam : 학습률을 파라미터별로 적응적으로 조절하는 방법으로, 현재 가장 널리 쓰이는 옵티마이저 중 하나


import numpy as np
import matplotlib.pyplot as plt

# 데이터 생성
np.random.seed(42)
x = np.random.randn(100)
y = 2 * x + 3 + np.random.randn(100) * 0.5 # y = 2x + 3 + 노이즈

# 파라미터 초기화
w = 0.0
b = 0.0
lr = 0.1
epochs = 100

# 기록용
loss_history = []
w_history = []
b_history = []

# 경사 하강법
for epoch in range(epochs):
    # 예측
    y_pred = w * x + b

    # 손실 계산(MSE)
    loss = np.mean((y - y_pred) ** 2)
    loss_history.append(loss)

    # Gradient 계산
    dw = np.mean(2 * (y_pred - y) * x)
    db = np.mean(2 * (y_pred - y))
    
    # 파라미터 업데이트
    w = w - lr * dw
    b = b - lr * db

    # 기록
    w_history.append(w)
    b_history.append(b)

    if epoch % 20 == 0:
        print(f'Epoch {epoch:3d}: Loss={loss:.4f}, w={w:.4f}, b={b:.4f}')

print(f'최종: w={w:.4f} (목표: 2), b={b:.4f} (목표: 3)')


