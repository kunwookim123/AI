# 인공 신경망의 구조

# 생물학적 뉴런 vs 인공 뉴런
# 생물학적 뉴런
# 구조
# 수상돌기 -> 세포체 -> 축삭 -> 시냅스

# 동작
# 수상돌기 : 다른 뉴런에서 신호 수신
# 세포체 : 신호 합산 및 처리
# 축삭 : 신호 전달
# 시냅스 : 다음 뉴런으로 신호 전송

# 특징
# 임계값 이상이면 발화(firing)
# 약 860억 개의 뉴런
# 100조 개의 시냅스 연결


# 인공 뉴런(퍼셉트론)
# 입력 (x₁, x₂, ..., xₙ) (특징)
#     ↓
# 가중치 곱 (w₁x₁ + w₂x₂ + ... + wₙxₙ) (입력의 중요도) -> 시냅스 강도처럼
#     ↓
# 편향 더하기 (+b) (기준점 조절) -> 임계값을 옮기는 효과
#     ↓
# 활성화 함수 f( ) '얼마나 켤지' 결정
#     ↓
# 출력 y

# 수식: y = f(Σwᵢxᵢ + b)

# 한 줄 요약
# 생물학적 뉴런 : 전기, 화학 신호를 시간에 따라 통합하고, 임계치를 넘으면
# 발화하며 연결 강도가 변하면서 학습

# 인공 뉴런 : 입력을 가중합한 두 활성화 함수를 통과시켜 출력하고,
# 오차를 줄이도록 가중치를 업데이트하며 학습

# 신경망의 층 구조
# 층의 종류

# 입력층(Input Layer)
# 데이터를 받아들이는 층
# 특성 개수 = 뉴런 개수
# 계산 없음, 단순 전달

# 은닉층(Hidden Layer)
# 입력과 출력 사이의 층
# 특성 추출 및 반환
# 1개 이상(딥러닝 = 많은 은닉층)

# 출력층(Output Layer)
# 최종 예측 결과
# 작업에 따라 뉴런 개수 결정

import torch.nn as nn

model = nn.Sequential(
    nn.Linear(4,3), # 입력층 -> 은닉층1 (4->3)
    nn.ReLU(),      # 활성화
    nn.Linear(3,3), # 은닉층1 -> 은닉층2 (3->3)
    nn.ReLU(),      # 활성화
    nn.Linear(3,2)  # 은닉층2 -> 출력층 (3->2)
)

print(model)

# 가중치와 편향
# 가중치 = 연결의 강도

# 역할
# 입력의 중요도를 결정
# 양수 : 입력과 같은 방향
# 음수 : 입력과 반대 방향
# 0에 가까움 : 영향이 적음

# 학습 대상:
# 초기 : 무작위 초기화
# 학습 : 역전파로 업데이트

# 편향

# 역할
# 활성화 임계값 조절
# 입력이 0일 때도 출력 가능
# 유연성 증가

# 수식 비교
# y = wx (편향 없음) -> 원점 통과
# y = wx + b (편향 있음) -> y절편 = b

# 파라미터 개수 계산
# Linear 층의 파라미터 개수 = (입력 크기 x 출력 크기) + 출력 크기
#                         = 가중치 개수 + 편향 개수

# Linear(10,5)
# 가중치 : 10 x 5 = 50개(입력 10개와 출력 5개와의 연결)
# 편향 : 5개(각 출력 뉴런마다 1개)
# 총 50 + 5 = 55개

import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(4,3), # 입력층 -> 은닉층1 (4->3) => 15
    nn.ReLU(),      # 활성화
    nn.Linear(3,3), # 은닉층1 -> 은닉층2 (3->3) => 12
    nn.ReLU(),      # 활성화
    nn.Linear(3,2)  # 은닉층2 -> 출력층 (3->2) => 8 총 35
) 

# 총 파라미터 개수
total_params = sum(p.numel() for p in model.parameters())
print(total_params)

# 층별 파라미터 개수
for name, param in model.named_parameters():
    print(f'{name}: {param.shape}')


# 순전파 (Forward Propagation)
# 입력 -> 출력 방향으로 계산 진행

# 과정
# 1. 입력 데이터 받기
# 2. 각 층에서 가중합 계산: z = Wx + b
# 3. 활성화 함수 적용: a = f(z)
# 4. 다음 층으로 전달
# 5. 출력층까지 반복

# 2층 신경망:
# 층1: z₁ = W₁x + b₁
#      a₁ = f(z₁)

# 층2: z₂ = W₂a₁ + b₂
#      a₂ = f(z₂)

# 출력: ŷ = a₂


# 역전파 (Backpropagation)
# 정답과 오차(손실)가 줄어들도록 가중치w, 편향b를 어떻게 바꿀지를 미분(체인룰)로
# 계산해서, 각 층에 '너는 얼마나 책임이 있니?'를 나눠주는 방법

# 순전파: 입력 -> 층들 통과 -> 예측값

# 왜 '거꾸로' 가냐?
# 출력 오차는 출력층 파라미터와 가장 직접적으로 연결
# 그리고 그 오차가 그 전 층의 출력 때문에 생긴 부분도 있고,
# 더 전 층 때문에 생긴 부분도 있음

# 그래서 '오차를 만든 책임'을 출력층에서 시작해 이전 층으로 분배해야 하는데,
# 이때 쓰는 게 체인 룰입니다.


# 순전파 과정 예시
# 1층 신경망 예시:

# 입력(x) => 가중치 곱하기 -> 편향 더하기 -> 활성화 -> 출력(ŷ)

# 수식
# z = w × x + b
# ŷ = σ(z)  (σ는 활성화 함수, 예: sigmoid)

# 손실
# L = (ŷ - y)²  (y는 실제 값)


# 구체적 예시
# x = 2.0
# w = 0.5
# b = 1.0
# y = 3.0 (실제 타겟)

# 순전파 계산:
# 1. z = w × x + b
#      = 0.5 × 2.0 + 1.0
#      = 2.0

# 2. ŷ = σ(z)
#      = 1 / (1 + e^(-2.0))
#      ≈ 0.88

# 3. L = (ŷ - y)²
#      = (0.88 - 3.0)²
#      = (-2.12)²
#      ≈ 4.49

# 연쇄 법칙(Chain Rule)
# 함수가 중첩되어 있을 때 미분하는 방법

# 예: y = f(g(x))

# 미분:
# dy/dx = (dy/dg) × (dg/dx)

# 읽는 법
# y를 x로 미분 = y를 g로 미분 × g를 x로 미분



# 예: y = (2x + 3)²

# 방법 1: 직접 전개해서 미분
# y = 4x² + 12x + 9
# dy/dx = 8x + 12

# 방법 2: 연쇄 법칙 (중간 변수 사용)
# u = 2x + 3 =>  du/dx = 2
# y = u² =>  dy/du = 2u

# dy/du = 2u
# du/dx = 2

# dy/dx = (dy/du) × (du/dx)
#       = 2u × 2
#       = 2(2x + 3) × 2
#       = 4(2x + 3)
#       = 8x + 12  ← 같은 결과!

# x = 1일 때:
# dy/dx = 8(1) + 12 = 20

# 왜 연쇄 법칙을 쓰는가?
# 신경망은 함수의 중첩 구조:
# 출력 = f₅(f₄(f₃(f₂(f₁(입력)))))

# 직접 미분? -> 너무 복잡
# 연쇄 법칙? -> 가능! (단계별로 곱하면 됨)

# ∂출력/∂입력 = (∂출력/∂f₅) × (∂f₅/∂f₄) × (∂f₄/∂f₃) × (∂f₃/∂f₂) × (∂f₂/∂f₁) × (∂f₁/∂입력)


# 가장 단순한 경우:
# x → [w곱하기] → z → [제곱] → L

# 수식:
# z = w × x
# L = z²

# 목표: ∂L/∂w 구하기


# 예시
# x = 3
# w = 2

# z = w × x = 2 × 3 = 6
# L = z² = 36

# 역전파 (연쇄 법칙):
# ∂L/∂w = (∂L/∂z) × (∂z/∂w)

# 1. ∂L/∂z = 2z = 2 × 6 = 12
#    (L = z²를 z로 미분)

# 2. ∂z/∂w = x = 3
#    (z = w × x를 w로 미분)

# 3. ∂L/∂w = 12 × 3 = 36

# 해석:
# w가 1 증가하면 -> L이 약 36 증가
# w를 줄여야 L이 감소!


# 경사하강법 적용:
# w_new = w_old - η × (∂L/∂w)

# η = 0.01 (학습률)
# w_old = 2
# ∂L/∂w = 36

# w_new = 2 - 0.01 × 36 = 1.64

# 검증 (순전파):
# z = w × x = 1.64 × 3 = 4.92
# L = z² = 4.92² = 24.21

# 손실이 36 -> 24.21로 감소!